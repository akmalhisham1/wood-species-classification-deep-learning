Model Architecture

This document explains the structure and logic behind the two deep learning models used in this project.

1. Xception Model

Xception (Extreme Inception) is a deep CNN architecture that improves both speed and accuracy by replacing traditional convolutions with depthwise separable convolutions.

ğŸ”§ Key Ideas

Depthwise convolution â†’ learns spatial patterns

Pointwise convolution â†’ learns channel (feature map) relationships

Fewer parameters, faster training

Excellent for fine-grained image classification (perfect for wood textures)

ğŸ— Architecture Used in This Project

Input â†’ Xception base (pretrained on ImageNet)

Output feature map: (3 Ã— 3 Ã— 2048)

Flatten layer

Dense layer (4 neurons â€“ Softmax)

Only the Dense layer is trainable (73k parameters)

Total parameters: 20.9 million

ğŸ¯ Why It Works Well

Wood textures have fine patterns â†’ Xception excels at capturing subtle details

Highest accuracy: 87%

2. ResNet50 Model

ResNet50 introduces the concept of residual connections, which help extremely deep networks learn effectively by avoiding the â€œvanishing gradientâ€ problem.

ğŸ”§ Key Ideas

Skip connections allow gradients to flow through the network

50-layer deep architecture

Strong baseline for most image-classification tasks

Powerful, but heavier than Xception

ğŸ— Architecture Used

Input â†’ ResNet50 base (pretrained on ImageNet)

Output: (3 Ã— 3 Ã— 2048)

Flatten layer

Dense layer (4 neurons â€“ Softmax)

Only Dense layer is trainable

Total parameters: 23.6 million

âš  Observation

Shows slight overfitting

Accurate but less robust

Final accuracy: 82%

3. Comparison Summary
Aspect	Xception	ResNet50
Architecture style	Depthwise separable CNN	Residual CNN
Parameters	20.9M	23.6M
Trainable params	73k	73k
Performance	â­ Best (87%)	Good (82%)
Overfitting	Low	Medium
Suitable for	Fine-grained textures	General image tasks
4. Why Xception Was Chosen

âœ” Captures detailed texture patterns
âœ” More efficient
âœ” Better generalization
âœ” Higher precision, recall, and F1-score across input classes

5. Implementation Notes

Both models use transfer learning, freezing the pretrained layers

Only the classification layer is trained

Works well even with smaller datasets

Maintains fast training time even on CPU/GPU
